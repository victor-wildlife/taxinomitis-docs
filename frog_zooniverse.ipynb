{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frog_zooniverse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOJ7A4sQiGlZyNvFY82WEWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-wildlife/taxinomitis-docs/blob/master/frog_zooniverse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvs6P42y4QT0"
      },
      "source": [
        "This notebook contains the scripts to upload photos of Archey's frogs to a Zooniverse project and download labels of the landmarks of the frogs to train ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcT2UBUcdj-a"
      },
      "source": [
        "#Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH2C_1HBdmX4"
      },
      "source": [
        "We use the \"panoptes_client\" package to communicate with Zooniverse. If you don't have it installed, run the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2E5PR5iaGfC"
      },
      "source": [
        "!pip install panoptes_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBrZIug5d896"
      },
      "source": [
        "Load generic libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYnsIFqAeBOT"
      },
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "from panoptes_client import (\n",
        "    SubjectSet,\n",
        "    Subject,\n",
        "    Project,\n",
        "    Panoptes,\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w56RZpfq43Gj"
      },
      "source": [
        "# Download frog photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW-EFyImA8fy"
      },
      "source": [
        "###Add shortcuts to the compressed photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdLfczSGA9aL"
      },
      "source": [
        "To download the photos of the frogs into this Google Colab you first need to add shortcuts in your Google drive to the [five zipped folders](https://drive.google.com/file/d/1XXSrATFX1l-J0CUE4m6UfoOBp9zv3XOr/view?usp=sharing) with the photos. \n",
        "\n",
        "To add the shortcuts:\n",
        "* go to the \"Shared with me\" section in your Google drive,\n",
        "* find the five zipped folders,\n",
        "* click on \"Add shorcut to Drive\" and\n",
        "* save the shortcuts (we created a folder called \"frog_photos\" and saved them there)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4n9rLaUBIaQ"
      },
      "source": [
        "*Specify* the folder in your Google drive where you saved the shortcuts to the photos (in our case \"frog_photos\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-mA0G4XBEti"
      },
      "source": [
        "dir_shortcuts = \"/content/drive/My Drive/frog_photos/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPDH9kwjBOr5"
      },
      "source": [
        "*If you can't access the five zipped folders please [email Victor](victor@wildlife.ai). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJcn0ys34ycz"
      },
      "source": [
        "###Download the compressed photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RPy3EYdBPo7"
      },
      "source": [
        "To download the five zip folders with the photos you will need to grant access to the Google file stream. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWsmRVDA4I5Y",
        "outputId": "58ab188d-8a0d-4e11-a266-2fc5aa5d101d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "drive.mount('/content/drive/')\n",
        "\n",
        "whareorino_a = zipfile.ZipFile(dir_shortcuts + \"whareorino_a.zip\", 'r')\n",
        "whareorino_a_pd = pd.DataFrame(\n",
        "    [x for x in whareorino_a.namelist() if 'Individual Frogs' in x and not x.endswith(('.db','/','Store'))]\n",
        ")\n",
        "\n",
        "whareorino_b = zipfile.ZipFile(dir_shortcuts + \"whareorino_b.zip\", 'r')\n",
        "whareorino_b_pd = pd.DataFrame(\n",
        "    [x for x in whareorino_b.namelist() if 'Individual Frogs' in x and not x.endswith(('.db','/','Store'))]\n",
        ")\n",
        "whareorino_c = zipfile.ZipFile(dir_shortcuts + \"whareorino_c.zip\", 'r')\n",
        "whareorino_c_pd = pd.DataFrame(\n",
        "    [x for x in whareorino_c.namelist() if 'Individual Frogs' in x and not x.endswith(('.db','/','Store'))]\n",
        ")\n",
        "\n",
        "whareorino_d = zipfile.ZipFile(dir_shortcuts + \"whareorino_d.zip\", 'r')\n",
        "whareorino_d_pd = pd.DataFrame(\n",
        "    [x for x in whareorino_d.namelist() if 'Individual Frogs' in x and not x.endswith(('.db','/','Store'))]\n",
        ")\n",
        "pukeokahu = zipfile.ZipFile(dir_shortcuts + \"pukeokahu.zip\", 'r')\n",
        "pukeokahu_pd = pd.DataFrame(\n",
        "    [x for x in pukeokahu.namelist() if 'Individual Frogs' in x and not x.endswith(('.db','/','Store'))]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_rviwZICUxy"
      },
      "source": [
        "#Create a data frame with frog information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Adg9vOm4yl8"
      },
      "source": [
        "Create a data frame to keep track of the photos uploaded to Zooniverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKgOvCsEX3lI"
      },
      "source": [
        "###Prepare information related to the photos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Gp-086GaCn",
        "outputId": "f23603e7-0b1c-4768-e194-cdce4c90c160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Combine the different grids into a single data frame\n",
        "pdList = [whareorino_a_pd,\n",
        "          whareorino_b_pd,\n",
        "          whareorino_c_pd,\n",
        "          whareorino_d_pd,\n",
        "          pukeokahu_pd]\n",
        "\n",
        "frog_df = pd.concat(pdList)\n",
        "\n",
        "#Rename the column\n",
        "frog_df = frog_df.rename(columns={0: \"file_path\"})\n",
        "\n",
        "#Add new columns based on the directory and filename of the photos\n",
        "directories = frog_df['file_path'].str.split(\"/\", n = 4, expand = True)\n",
        "\n",
        "# making separate first name column from new data frame \n",
        "frog_df[\"grid\"] = directories[0]\n",
        "frog_df[\"frog_id\"] = directories[2] \n",
        "frog_df[\"filename\"] = directories[3] \n",
        "\n",
        "frog_df[\"capture\"] = frog_df[\"filename\"].str.split(\".\",1, expand = True)[0].str.replace('_', '-').str.rsplit(\"-\",1, expand = True)[0] \n",
        "\n",
        "#Sort the columns to match the database\n",
        "frog_df = frog_df[\n",
        "        [\"filename\", \"file_path\", \"capture\" , \"frog_id\", \"grid\"]\n",
        "    ]\n",
        "                                      \n",
        "frog_df[\"subject_id\"] = np.nan\n",
        "#list(frog_df.columns)\n",
        "#frog_df.iloc[1442]['filename']\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated photos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMJh8aydX8pu"
      },
      "source": [
        "###Prepare information related to Zooniverse subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUh4bvCGfPvt"
      },
      "source": [
        "You need to specify your Zooniverse username and password. Uploading and downloading information from Zooniverse is only accessible to those user with access to the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8sbIcMBfMTq"
      },
      "source": [
        "zoo_user = \"usern\"\n",
        "zoo_pass = \"pass\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNxL6SRwYb0R",
        "outputId": "b4764e1c-a3c7-449e-b1cb-8fa2c0a19865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "# Connect to Zooniverse with your username and password\n",
        "auth = Panoptes.connect(username=zoo_user, password=zoo_pass)\n",
        "\n",
        "if not auth.logged_in:\n",
        "    raise AuthenticationError(\"Your credentials are invalid. Please try again.\")\n",
        "\n",
        "# Connect to the Zooniverse project (our frog project # is 13355)\n",
        "project = Project(13355)\n",
        "\n",
        "# Get info of subjects uploaded to the project\n",
        "export = project.get_export(\"subjects\")\n",
        "\n",
        "# Save the subjects info as pandas data frame\n",
        "subjects_df = pd.read_csv(\n",
        "    io.StringIO(export.content.decode(\"utf-8\")),\n",
        "    usecols=[\n",
        "        \"subject_id\",\n",
        "        \"metadata\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Reset index of df\n",
        "subj_df = subjects_df.reset_index(drop=True).reset_index()\n",
        "\n",
        "# Flatten the metadata from the uploaded subjects\n",
        "meta_df = pd.json_normalize(subj_df.metadata.apply(json.loads))\n",
        "\n",
        "# Drop metadata and index columns from original df\n",
        "subj_df = subj_df.drop(columns=[\"metadata\", \"index\",]).rename(\n",
        "    columns={\"id\": \"subject_id\"}\n",
        ")\n",
        "\n",
        "# Add the subject_id of photos already uploaded to Zooniverse\n",
        "frog_df = pd.merge(frog_df, subj_df, how=\"left\", on=\"movie_filename\")\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "PanoptesAPIException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPanoptesAPIException\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-2f91037adbf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Get info of subjects uploaded to the project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mexport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subjects\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Save the subjects info as pandas data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/panoptes_client/exportable.py\u001b[0m in \u001b[0;36mget_export\u001b[0;34m(self, export_type, generate, wait, wait_timeout)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mexport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mexport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexport_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTALK_EXPORT_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/panoptes_client/exportable.py\u001b[0m in \u001b[0;36mdescribe_export\u001b[0;34m(self, export_type)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         return self.http_get(\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         )[0]\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/panoptes_client/panoptes.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(cls, path, params, headers, retry, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/panoptes_client/panoptes.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, params, headers, endpoint, retry)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         )\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/panoptes_client/panoptes.py\u001b[0m in \u001b[0;36mjson_request\u001b[0;34m(self, method, path, params, headers, json, etag, endpoint, retry)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 raise PanoptesAPIException(', '.join(\n\u001b[1;32m    280\u001b[0m                     map(lambda e: e.get('message', ''),\n\u001b[0;32m--> 281\u001b[0;31m                         \u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                        )\n\u001b[1;32m    283\u001b[0m                 ))\n",
            "\u001b[0;31mPanoptesAPIException\u001b[0m: No subjects_export exists for project #13355"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0xYgC4-c4E3"
      },
      "source": [
        "#Upload new photos to Zooniverse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxiobTeZgSZ1"
      },
      "source": [
        "#Select n number of photos to upload to Zooniverse\n",
        "photos_upload = frog_df[frog_df['subject_id'].isnull()].sample(n = 3)\n",
        "\n",
        "# Rename the columns that will appear as metadata associated to the subjects\n",
        "photos_upload = photos_upload[\n",
        "            [\n",
        "             \"file_path\",\n",
        "             \"filename\", \n",
        "             \"capture\" , \n",
        "             \"frog_id\", \n",
        "             \"grid\"\n",
        "            ]\n",
        "        ]\n",
        "        \n",
        "# Save the df as the subject metadata\n",
        "subject_metadata = sp_frames_df.set_index('frame_path').to_dict('index')\n",
        "\n",
        "  # Create a subjet set in Zooniverse to host the frames\n",
        "subject_set = SubjectSet()\n",
        "\n",
        "subject_set.links.project = koster_project\n",
        "subject_set.display_name = args.species + date.today().strftime(\"_%d_%m_%Y\")\n",
        "\n",
        "subject_set.save()\n",
        "\n",
        "print(\"Zooniverse subject set created\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD2YuMTzgPN6"
      },
      "source": [
        "import argparse, os, cv2, re\n",
        "import utils.db_utils as db_utils\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pims\n",
        "\n",
        "from PIL import Image\n",
        "from datetime import date\n",
        "from utils.zooniverse_utils import auth_session\n",
        "from panoptes_client import (\n",
        "    SubjectSet,\n",
        "    Subject,\n",
        "    Project,\n",
        "    Panoptes,\n",
        ")\n",
        "\n",
        "# Function to identify up to n number of frames per classified clip\n",
        "# that contains species of interest after the first time seen\n",
        "def get_species_frames(species_id, conn, n_frames):\n",
        "\n",
        "    # Find classified clips that contain the species of interest\n",
        "    frames_df = pd.read_sql_query(\n",
        "        f\"SELECT subject_id, first_seen FROM agg_annotations_clip WHERE agg_annotations_clip.species_id={species_id}\",\n",
        "        conn,\n",
        "    )\n",
        "\n",
        "    # Add species id to the df\n",
        "    frames_df[\"frame_exp_sp_id\"] = species_id\n",
        "    \n",
        "    # Get start time of the clips and ids of the original movies\n",
        "    (frames_df[\"clip_start_time\"], frames_df[\"movie_id\"],) = list(\n",
        "        zip(\n",
        "            *pd.read_sql_query(\n",
        "                f\"SELECT clip_start_time, movie_id FROM subjects WHERE id IN {tuple(frames_df['subject_id'].values)} AND subject_type='clip'\",\n",
        "                conn,\n",
        "            ).values\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Identify the second of the original movie when the species first appears\n",
        "    frames_df[\"first_seen_movie\"] = frames_df[\"clip_start_time\"] + frames_df[\"first_seen\"]\n",
        "\n",
        "    # Get the filepath and fps of the original movies\n",
        "    f_paths = pd.read_sql_query(f\"SELECT id, fpath, fps FROM movies\", conn)\n",
        "\n",
        "    # Ensure swedish characters don't cause issues\n",
        "    f_paths[\"fpath\"] = f_paths[\"fpath\"].apply(\n",
        "        lambda x: str(x) if os.path.isfile(str(x)) else db_utils.unswedify(str(x))\n",
        "    )\n",
        "    \n",
        "    # Include movies' filepath and fps to the df\n",
        "    frames_df = frames_df.merge(f_paths, left_on=\"movie_id\", right_on=\"id\")\n",
        "    \n",
        "    # Specify if original movies can be found\n",
        "    frames_df['exists'] = frames_df['fpath'].map(os.path.isfile)\n",
        "    \n",
        "    if len(frames_df[~frames_df.exists]) > 0:\n",
        "        print(\n",
        "            f\"There are {len(frames_df) - frames_df.exists.sum()} out of {len(frames_df)} frames with a missing movie\"\n",
        "        )\n",
        "        \n",
        "    # Select only frames from movies that can be found\n",
        "    frames_df = frames_df[frames_df.exists]\n",
        "    \n",
        "    # Identify the ordinal number of the frames expected to be extracted\n",
        "    frames_df[\"frame_number\"] = frames_df[\n",
        "        [\"first_seen_movie\", \"fps\"]\n",
        "    ].apply(\n",
        "        lambda x: [\n",
        "            int((x[\"first_seen_movie\"] + j) * x[\"fps\"])\n",
        "            for j in range(n_frames)\n",
        "        ],\n",
        "        1,\n",
        "    )\n",
        "    \n",
        "    # Reshape df to have each frame as rows\n",
        "    lst_col = 'frame_number'\n",
        "\n",
        "    frames_df = pd.DataFrame({\n",
        "        col:np.repeat(frames_df[col].values, frames_df[lst_col].str.len())\n",
        "        for col in frames_df.columns.difference([lst_col])\n",
        "    }).assign(**{lst_col:np.concatenate(frames_df[lst_col].values)})[frames_df.columns.tolist()]\n",
        "    \n",
        "    # Drop unnecessary columns\n",
        "    frames_df.drop([\"subject_id\"], inplace=True, axis=1)\n",
        "    \n",
        "    return frames_df\n",
        "    \n",
        "# Function to extract frames \n",
        "def extract_frames(df, frames_folder):    \n",
        "\n",
        "    # Get movies filenames from their path\n",
        "    df[\"movie_filename\"] = df[\"fpath\"].str.split('/').str[-1].str.replace(\".mov\", \"\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Set the filename of the frames\n",
        "    df[\"frame_path\"] = (frames_folder\n",
        "                      + df[\"movie_filename\"].astype(str)\n",
        "                      + \"_frame_\"\n",
        "                      + df[\"frame_number\"].astype(str)\n",
        "                      + \"_\"\n",
        "                      + df[\"frame_exp_sp_id\"].astype(str)\n",
        "                      + \".jpg\"\n",
        "                     )\n",
        "    \n",
        "    \n",
        "    # Read all original movies\n",
        "    video_dict = {k: pims.Video(k) for k in df[\"fpath\"].unique()}\n",
        "\n",
        "    # Save the frame as matrix    \n",
        "    df[\"frames\"] = df[[\"fpath\", \"frame_number\", \"fps\"]].apply(\n",
        "        lambda x: video_dict[x[\"fpath\"]][\n",
        "            np.arange(\n",
        "                int(x[\"frame_number\"]),\n",
        "                int(x[\"frame_number\"])\n",
        "                    + int(x[\"fps\"]),\n",
        "                int(x[\"fps\"]),\n",
        "            )\n",
        "        ],\n",
        "        1,\n",
        "    )\n",
        "    \n",
        "    # Extract and save frames\n",
        "    for frame, filename in zip(df[\"frames\"].explode(), df[\"frame_path\"].explode()):\n",
        "        Image.fromarray(frame).save(f\"{filename}\")\n",
        "        \n",
        "    print(\"Frames extracted successfully\")\n",
        "    return df[\"frame_path\"]\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    \"Handles argument parsing and launches the correct function.\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--user\", \"-u\", help=\"Zooniverse username\", type=str, required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--password\", \"-p\", help=\"Zooniverse password\", type=str, required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--species\", \"-l\", help=\"Species to upload\", type=str, required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-db\",\n",
        "        \"--db_path\",\n",
        "        type=str,\n",
        "        help=\"the absolute path to the database file\",\n",
        "        default=r\"koster_lab.db\",\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-fp\",\n",
        "        \"--frames_folder\",\n",
        "        type=str,\n",
        "        help=\"the absolute path to the folder to store frames\",\n",
        "        default=r\"./frames\",\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-t\",\n",
        "        \"--testing\",\n",
        "        help=\"add flag if testing\",\n",
        "        required=False,\n",
        "        action=\"store_true\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-nf\",\n",
        "        \"--n_frames\",\n",
        "        type=int,\n",
        "        help=\"number of frames to create per clip\",\n",
        "        default=2,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Connect to koster_db\n",
        "    conn = db_utils.create_connection(args.db_path)\n",
        "\n",
        "    # Connect to Zooniverse\n",
        "    koster_project = auth_session(args.user, args.password)\n",
        "\n",
        "    # Get id of species of interest\n",
        "    species_id = pd.read_sql_query(\n",
        "        f\"SELECT id FROM species WHERE label='{args.species}'\", conn\n",
        "    ).values[0][0]\n",
        "\n",
        "    # Identify n number of frames per classified clip that contains species of interest \n",
        "    sp_frames_df = get_species_frames(species_id, conn, args.n_frames)\n",
        "\n",
        "    # Get info of frames already uploaded\n",
        "    uploaded_frames_df = pd.read_sql_query(\n",
        "        f\"SELECT movie_id, frame_number, frame_exp_sp_id FROM subjects WHERE frame_exp_sp_id='{species_id}' and subject_type='frame'\",\n",
        "        conn,\n",
        "    )\n",
        "\n",
        "    # Filter out frames that have already been uploaded\n",
        "    if len(uploaded_frames_df) > 0 and not args.testing:\n",
        "\n",
        "        # Exclude frames that have already been uploaded\n",
        "        sp_frames_df = sp_frames_df[\n",
        "            ~(sp_frames_df[\"movie_id\"].isin(uploaded_frames_df[\"movie_id\"]))\n",
        "            & ~(sp_frames_df[\"frame_number\"].isin(uploaded_frames_df[\"frame_number\"]))\n",
        "            & ~(\n",
        "                sp_frames_df[\"frame_exp_sp_id\"].isin(\n",
        "                    uploaded_frames_df[\"frame_exp_sp_id\"]\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Upload frames to Zooniverse that have not been uploaded\n",
        "    if len(sp_frames_df) == 0:\n",
        "        print(\n",
        "            \"There are no subjects to upload, this may be because all of the subjects have already been uploaded\"\n",
        "        )\n",
        "        raise\n",
        "     \n",
        "    else:\n",
        "        # Create the folder to store the frames if not exist\n",
        "        if not os.path.exists(args.frames_folder):\n",
        "            os.mkdir(args.frames_folder)\n",
        "        \n",
        "        # Extract the frames and save them\n",
        "        sp_frames_df[\"frame_path\"] = extract_frames(sp_frames_df, args.frames_folder)\n",
        " \n",
        "        # Select koster db metadata associated with each frame\n",
        "        sp_frames_df[\"label\"] = args.species\n",
        "        sp_frames_df[\"subject_type\"] = \"frame\"\n",
        "\n",
        "        sp_frames_df = sp_frames_df[\n",
        "            [\n",
        "                \"frame_path\",\n",
        "                \"fpath\",\n",
        "                \"frame_number\",\n",
        "                \"fps\",\n",
        "                \"movie_id\",\n",
        "                \"label\",\n",
        "                \"frame_exp_sp_id\",\n",
        "                \"subject_type\",\n",
        "            ]\n",
        "        ]\n",
        "        \n",
        "        # Save the df as the subject metadata\n",
        "        subject_metadata = sp_frames_df.set_index('frame_path').to_dict('index')\n",
        "        \n",
        "         # Create a subjet set in Zooniverse to host the frames\n",
        "        subject_set = SubjectSet()\n",
        "\n",
        "        subject_set.links.project = koster_project\n",
        "        subject_set.display_name = args.species + date.today().strftime(\"_%d_%m_%Y\")\n",
        "\n",
        "        subject_set.save()\n",
        "\n",
        "        print(\"Zooniverse subject set created\")\n",
        "        \n",
        "        \n",
        "        # Upload frames to Zooniverse (with metadata)\n",
        "        new_subjects = []\n",
        "\n",
        "        for filename, metadata in subject_metadata.items():\n",
        "            subject = Subject()\n",
        "\n",
        "            subject.links.project = koster_project\n",
        "            subject.add_location(filename)\n",
        "\n",
        "            subject.metadata.update(metadata)\n",
        "\n",
        "            subject.save()\n",
        "            new_subjects.append(subject)\n",
        "\n",
        "        # Upload frames\n",
        "        subject_set.add(new_subjects)\n",
        "        \n",
        "        print(\"Subjects uploaded to Zooniverse\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ9HlEUpc-Gq"
      },
      "source": [
        "#Download Zooniverse annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGDF3PBcYEMf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfLReh-mYCxp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7c1GcYwKRon"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "\n",
        "# Create a df of the photos found in the tmp folder\n",
        "data = []\n",
        "# Loop through each folder in the tmp directory\n",
        "for grid in os.listdir('../tmp/'):\n",
        "  if 'Grid' in grid:\n",
        "    grid_path = '../tmp/' + grid\n",
        "    # Loop through each subfolder in the 'Grid' directories\n",
        "    for subfolder in os.listdir(grid_path):\n",
        "      if 'Individual' in subfolder:\n",
        "        subfolder_path = grid_path + \"/\" + subfolder\n",
        "        # Loop through each individual frog in the \"individual frog\" directoy\n",
        "        for ind in os.listdir(subfolder_path):\n",
        "          if not ind.endswith('db'):\n",
        "            ind_path = subfolder_path + \"/\" + ind\n",
        "            # Loop through each photo of the \"individual\" frog\n",
        "            for doc in os.listdir(ind_path):\n",
        "              #Save information about the photo and the frog\n",
        "              if not doc.endswith('db'):\n",
        "                fpath = ind_path + \"/\" + doc\n",
        "                capt = doc.split(\".\",1)[0].replace('_', '-').rsplit(\"-\",1)[1]\n",
        "                data.append((doc, fpath, capt, ind, grid))\n",
        "\n",
        "df = pd.DataFrame(data,columns = ['filename', 'file_path', 'capture', 'frog_id', 'grid'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYTBTyV8JtfW"
      },
      "source": [
        "movies_df = db_utils.download_csv_from_google_drive(movies_file_id)\n",
        "\n",
        "    # Include server's path of the movie files\n",
        "    movies_df[\"Fpath\"] = movies_path + \"/\" + movies_df[\"FilenameCurrent\"] + \".mov\"\n",
        "\n",
        "    # Standarise the filename\n",
        "    movies_df[\"FilenameCurrent\"] = movies_df[\"FilenameCurrent\"].str.normalize(\"NFD\")\n",
        "    \n",
        "    # Set up sites information\n",
        "    sites_db = movies_df[\n",
        "        [\"SiteDecription\", \"CentroidLat\", \"CentroidLong\"]\n",
        "    ].drop_duplicates(\"SiteDecription\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}